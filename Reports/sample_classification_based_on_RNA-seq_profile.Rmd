---
title: "Sample classification based on RNA-seq profiles"
author: "Mustafa AbuElqumsan and Jacques van Helden"
date: '`r Sys.Date()`'
output:
  beamer_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: no
    keep_tex: no
    slide_level: 2
    theme: Montpellier
    toc: yes
  html_document:
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
  ioslides_presentation:
    colortheme: dolphin
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    fonttheme: structurebold
    highlight: tango
    incremental: yes
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: yes
    toc_depth: 3
  slidy_presentation:
    fig_caption: yes
    fig_height: 6
    fig_width: 7
    highlight: tango
    incremental: yes
    keep_md: yes
    smaller: yes
    theme: cerulean
    toc: yes
    widescreen: yes
  word_document:
    toc: yes
    toc_depth: '3'
font-import: http://fonts.googleapis.com/css?family=Risque
font-family: Garamond
transition: linear
---

```{r include=FALSE, echo=FALSE, eval=TRUE}
options(width=300)
knitr::opts_chunk$set(
  fig.width = 7, fig.height = 5, 
  fig.path='figures',
  fig.align = "center", 
  size = "tiny", 
  echo = TRUE, eval=TRUE, 
  warning = FALSE, message = FALSE, 
  results = TRUE, comment = "")
```

# Abstract 

# Introduction

RNA Sequqncing is a method relying on the next generation Sequencing (NGS) technologies to chracterize and quantify RNA from the Biological samples.
Classification is used to specify a category of new individuals (samples) based on a classification model built from predefined class labe of the individuals from thier trascriptom profile. the classification is categorized as suprevised learning method as it employs known answers ( class Labels) of the samples to predict the answer (label) of new individaul.   

# Motivation
## Biological motivation 
  predicte cancer class for the new individuals based on thier transcriptom profile.

## Methodological objectives
 - Evalute the accuracy of different classifier methods.
 - Generalize the power of the classifier (ability to correctly classify new individuals).
 - Robustness to sampling and variables variations.
  

# Materials and methods
 Train supervisied classifiers to assign individuals to predifined class labels relied on thier trascriptom profile.
 Supervised methods have scale invariant, quite rapid, low complexity may be parallelized and no normality assumption with RNA-Seq count data.
 
## pre-processing read counts 
Normalizing and Trasforming read counts to eliminate systematic effects that are not associates with the biological difference of interests.

## Quantifying the of reads per gene
## Raw count table
## Filtering Raw count from Zero Variance and near Zero Variance 
## dificulties with RNA-Seq counts

## Ordaring the genes in Raw count table according to differential Expression Analysis with respect to adj. P-values.

## computing miscalssification error rate for the supervised classification methods.

# Results and discussion
## Procedures to evaluate a classifier 
  There are three main algorithm for classification as in the following 
   - tree-based classification method using a traditional classification tree.
   - lazy-based algorithm as the K-nearest neghbour method that need to claculate the distance between observations
   - the hyperpalne algorithm that need to create the hyperplane separation between class of individuals.
   
 
### K-nearest Neighbours classifier 
we began with the k-nearst Neighbour method for testing it with high dimentionality data wherein that is nonparametric lazy learning method which does not make any assumption about the data distribution. what is the lazy learning mean? it doesn't need clear learning phase for generalisation.
- mechanism knn in classification process..
  knn train all samples and then classifies new individuals based on similarity (distance) measure, where the similarity measure can be formulated as follows:
  Euclidian distance is  $\sqrt {\sum\_{i=1}(x_{i}- y_{i})^2  $
  Manhattan distance is  $\sum \left\lvert  x_{i} - y_{i} \right\rvert$

In knn a new individual is classified to a label (class) that is common among the k-nearest neighbours. if k = 1, then the new individual is assigned to the class where its nearest neighbour belongs. the only required input for the algorithm is k. if we give a small k input, it would lead to over-fitting. if we give a large K input, it may result in under-fitting. we chosen a proper k-value eqaul 3 due to we perform $2/3$ proportion for training set and $1/3$ proportion for testing set.

- The advanteges of knn:
- The disadvantages of knn:

### Random forest classifier
 It is the famouse one from the ensemble learning method that is rely on arise multiple decision trees during the training process. Each decision tree will output its own prediction results corresponding to the input. and then the forest will use the voting mechanism to select the most voted class as the prediction result. 
 
 - mechanism random forest in classification process
 The objective of the random forest is to ensemble weak learners into a strong learner. wherein random forest firstly performs bootstrap sampling as the training dataset of each single decision tree. next in each node, the process first randomly select subset variables, then finds the predictor variable that provides the best split among subset variables. next, the process arise the full tree without pruning. in the end, we can obtain the predicted result of subset  from each single tree. as a result we get the prediction result by taking majority vote.

- The advanteges of random forest. it is easy to compute and can efficiently process data.
- The disadvantege is prone to over-fitting niosy data.

### Support vector machines (SVM) 
 SVM is powerfull classification tools by doing map of the individuals (samples) into high dimension space defined by the kernel function, and then find the optimum hyperplane that separates the idividuals by the maximum margin. in other mean we can conceive SVM as a linear algorithm in a high dimentional space.
 The bases of concept svm construct a hyperplane (or set of hyperplanes) that maximize the margin width between two classes in a high dimentional space, in these the cases that define the hyperplane are support vectors.
 where svm starts from building a hyperplane that maximizes the margin width. then it extends the definition to a nonlinear separable problem. finaly, it maps the samples to high dimensional space where the individuals can be more easily separated with a linear boundary.
 
 - "The advanteges of SVM it makes use of the regularization term to avoid over-fitting".

## Evaluation method by utlizing contingency table
 After we performing the classification process, we generated classification table "contengency Table" by using predicted classes in the row and the actual classes in the column. lastly we have obtained misclassification error rate from that Contengency Table where the intersection between rows and columns are correct classification and off diagonal are the misclassification error rate, we have used that misclassification error mesurment in order to exemplified the accuracy results between the three different mechanism for the classifiaction objective.
 
 BUT now we will go further to find other scalling to find out the accuracy between that methods by examine the  
 - specificity and sensitivity for each classifier 

## Impact of the number of variables into the performance of each classifiers.
as we can note from the next plot which explain how the classifier works with the number of variables (genes) are sotred according to the most significant by two tools for that are DESseq2 and edgeR which we used it of doing differential expresion analysis between the individual class and then we ordered all variables increasingly with the most significat consequntly we passing it to the each classifier to analysis the effect of number of variable into the qaulity of  performance of the classifiers   

## Study comparing between of all variables and all Principal component.
our main is to explian if the classifier will work in the same efficiany with all variable or with all principal component, where here we would exemplify the classifier efficieny will be better achive with individuals which is redueced thier dimensionality by the principal component tools for that purposes or it's better to work with individual in their nature. yo might find the misscalssification error rate for each classifier with two types from the data.

## Multidimensional scaling with PCA

## DEG-based variable (gene) ordering



# Conclusion

# Acknowledgements

# References








